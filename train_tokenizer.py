#!/usr/bin/env python3
"""
Train a HuggingFace-compatible BPE tokenizer with 256 byte tokens.
"""

import argparse
import os
import json
import time
from pathlib import Path

from tokenizers import Tokenizer, pre_tokenizers, processors, decoders
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.processors import TemplateProcessing
from transformers import PreTrainedTokenizerFast


def create_byte_level_bpe_tokenizer(
    vocab_size: int = 8192,
    special_tokens: list = None
) -> tuple[Tokenizer, BpeTrainer]:
    """
    åˆ›å»ºä¸€ä¸ªåŒ…å«256å­—èŠ‚ç çš„BPEåˆ†è¯å™¨ã€‚
    """
    if special_tokens is None:
        # 4ä¸ªç‰¹æ®Štoken
        special_tokens = ["<|pad|>", "<|eos|>", "<|bos|>", "<|unk|>"]
    
    # åˆå§‹åŒ– BPE æ¨¡å‹
    tokenizer = Tokenizer(BPE(unk_token="<|unk|>"))
    
    # ByteLevel pre-tokenizerï¼šè‡ªåŠ¨å¤„ç†256ä¸ªå­—èŠ‚ç 
    # add_prefix_space=False: ä¸åœ¨å¼€å¤´æ·»åŠ ç©ºæ ¼
    # use_regex=True: ä½¿ç”¨GPT-2é£æ ¼çš„æ­£åˆ™åˆ†å‰²
    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
        ByteLevel(add_prefix_space=False, use_regex=True)
    ])
    
    # ByteLevel decoder
    tokenizer.decoder = decoders.ByteLevel()
    
    # è®¡ç®—å®é™…éœ€è¦è®­ç»ƒçš„è¯è¡¨å¤§å°
    # 256 bytes + 4 special tokens = 260 å·²å ç”¨
    # å‰©ä½™ vocab_size - 260 ç”¨äº BPE merges
    num_byte_tokens = 256
    num_special = len(special_tokens)
    
    print(f"é…ç½®:")
    print(f"  - ç›®æ ‡è¯è¡¨å¤§å°: {vocab_size}")
    print(f"  - å­—èŠ‚ç æ•°é‡: {num_byte_tokens}")
    print(f"  - ç‰¹æ®Štokenæ•°é‡: {num_special}")
    print(f"  - BPEåˆå¹¶äº§ç”Ÿçš„token: {vocab_size - num_byte_tokens - num_special}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        show_progress=True,
        initial_alphabet=ByteLevel.alphabet(),  # 256 bytes
        min_frequency=2,  # è‡³å°‘å‡ºç°2æ¬¡æ‰åˆå¹¶
    )
    
    return tokenizer, trainer


def train_tokenizer(
    corpus_path: str,
    output_dir: str,
    vocab_size: int = 8192,
    special_tokens: list = None
):
    """
    è®­ç»ƒåˆ†è¯å™¨å¹¶ä¿å­˜ä¸ºHuggingFaceæ ¼å¼ã€‚
    """
    if special_tokens is None:
        special_tokens = ["<|pad|>", "<|eos|>", "<|bos|>", "<|unk|>"]
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒåˆ†è¯å™¨")
    print(f"{'='*60}")
    print(f"è¯­æ–™æ–‡ä»¶: {corpus_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è¯è¡¨å¤§å°: {vocab_size}")
    print(f"ç‰¹æ®Štokens: {special_tokens}")
    
    # è·å–è¯­æ–™æ–‡ä»¶å¤§å°
    corpus_size = os.path.getsize(corpus_path)
    print(f"è¯­æ–™å¤§å°: {corpus_size / 1024 / 1024:.2f} MB")
    
    # åˆ›å»ºåˆ†è¯å™¨å’Œè®­ç»ƒå™¨
    tokenizer, trainer = create_byte_level_bpe_tokenizer(
        vocab_size=vocab_size,
        special_tokens=special_tokens
    )
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    tokenizer.train(files=[corpus_path], trainer=trainer)
    
    elapsed = time.time() - start_time
    print(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {elapsed:.2f} ç§’")
    
    # æ·»åŠ åå¤„ç†å™¨ (å¯é€‰ï¼šæ·»åŠ  bos/eos)
    tokenizer.post_processor = TemplateProcessing(
        single="<|bos|> $A <|eos|>",
        pair="<|bos|> $A <|eos|> <|bos|> $B <|eos|>",
        special_tokens=[
            ("<|bos|>", tokenizer.token_to_id("<|bos|>")),
            ("<|eos|>", tokenizer.token_to_id("<|eos|>")),
        ],
    )
    
    # ä¿å­˜åº•å±‚ tokenizer.json
    tokenizer_json_path = output_path / "tokenizer.json"
    tokenizer.save(str(tokenizer_json_path))
    print(f"å·²ä¿å­˜: {tokenizer_json_path}")
    
    # åŒ…è£…ä¸º HuggingFace PreTrainedTokenizerFast
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        unk_token="<|unk|>",
        pad_token="<|pad|>",
        bos_token="<|bos|>",
        eos_token="<|eos|>",
        clean_up_tokenization_spaces=False,
    )
    
    # ä¿å­˜ä¸ºå®Œæ•´çš„ HuggingFace æ ¼å¼
    hf_tokenizer.save_pretrained(str(output_path))
    print(f"å·²ä¿å­˜ HuggingFace æ ¼å¼åˆ°: {output_path}")
    
    # éªŒè¯
    print(f"\n{'='*60}")
    print("éªŒè¯åˆ†è¯å™¨")
    print(f"{'='*60}")
    
    print(f"è¯è¡¨å¤§å°: {hf_tokenizer.vocab_size}")
    print(f"ç‰¹æ®Štokens: {hf_tokenizer.special_tokens_map}")
    
    # æµ‹è¯•ç¼–è§£ç 
    test_texts = [
        "Hello, world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "ğŸ‰ Emoji test ğŸš€",
        "Binary: \x00\x01\x02\xff",
        "Mixed: Helloä¸–ç•ŒğŸŒ",
    ]
    
    print("\nç¼–è§£ç æµ‹è¯•:")
    for text in test_texts:
        encoded = hf_tokenizer.encode(text)
        decoded = hf_tokenizer.decode(encoded)
        print(f"  åŸæ–‡: {repr(text)}")
        print(f"  ç¼–ç : {encoded[:20]}{'...' if len(encoded) > 20 else ''}")
        print(f"  è§£ç : {repr(decoded)}")
        print(f"  tokenæ•°: {len(encoded)}")
        print()
    
    # ä¿å­˜å…ƒä¿¡æ¯
    meta_info = {
        "vocab_size": hf_tokenizer.vocab_size,
        "special_tokens": special_tokens,
        "byte_tokens": 256,
        "corpus_size_mb": corpus_size / 1024 / 1024,
        "training_time_seconds": elapsed,
        "model_type": "BPE",
        "byte_level": True,
    }
    
    meta_path = output_path / "training_meta.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta_info, f, indent=2, ensure_ascii=False)
    print(f"å·²ä¿å­˜è®­ç»ƒå…ƒä¿¡æ¯: {meta_path}")
    
    # åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    for f in output_path.iterdir():
        size = f.stat().st_size
        print(f"  {f.name}: {size:,} bytes")
    
    return hf_tokenizer


def main():
    parser = argparse.ArgumentParser(description="Train a HuggingFace-compatible tokenizer")
    parser.add_argument("--corpus", type=str, required=True, help="Path to corpus file")
    parser.add_argument("--output", type=str, required=True, help="Output directory")
    parser.add_argument("--vocab-size", type=int, default=8192, help="Vocabulary size")
    parser.add_argument("--special-tokens", type=str, nargs="+", 
                        default=["<|pad|>", "<|eos|>", "<|bos|>", "<|unk|>"],
                        help="Special tokens")
    
    args = parser.parse_args()
    
    train_tokenizer(
        corpus_path=args.corpus,
        output_dir=args.output,
        vocab_size=args.vocab_size,
        special_tokens=args.special_tokens,
    )


if __name__ == "__main__":
    main()
