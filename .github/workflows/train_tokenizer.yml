name: Train Tokenizer

on:
  workflow_dispatch:
    inputs:
      vocab_size:
        description: 'Vocabulary size'
        required: false
        default: '8192'
      corpus_size_mb:
        description: 'Corpus size to use (MB)'
        required: false
        default: '500'

jobs:
  train:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Check available resources
        run: |
          echo "=== Disk Space ==="
          df -h
          echo ""
          echo "=== Memory ==="
          free -h
          echo ""
          echo "=== CPU ==="
          nproc

      - name: Install mimalloc
        run: |
          sudo apt-get update
          sudo apt-get install -y libmimalloc-dev libmimalloc2.0
          # 找到 mimalloc 库路径
          MIMALLOC_PATH=$(find /usr -name "libmimalloc.so*" 2>/dev/null | head -1)
          echo "MIMALLOC_PATH=$MIMALLOC_PATH" >> $GITHUB_ENV
          echo "Found mimalloc at: $MIMALLOC_PATH"

      # 创建 ramdisk - 只需要放截取后的语料，1.5GB 足够
      - name: Setup RAM disk
        run: |
          sudo mkdir -p /mnt/ramdisk
          sudo mount -t tmpfs -o size=1500M tmpfs /mnt/ramdisk
          df -h /mnt/ramdisk

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install gdown tokenizers transformers tqdm

      # 下载到普通磁盘（有足够空间）
      - name: Download corpus from Google Drive
        run: |
          FILE_ID="1uCexAXhCekAyTVCubvr0NtuliRlg7SBS"
          
          echo "Downloading to disk (not ramdisk)..."
          gdown --id "$FILE_ID" -O corpus_full.txt --fuzzy || \
          gdown "https://drive.google.com/uc?id=$FILE_ID" -O corpus_full.txt
          
          echo "Download complete:"
          ls -lh corpus_full.txt
          
          echo ""
          echo "Disk space after download:"
          df -h .

      # 截取并直接写入 ramdisk
      - name: Extract corpus to ramdisk
        run: |
          CORPUS_SIZE_MB=${{ github.event.inputs.corpus_size_mb || '500' }}
          CORPUS_SIZE_BYTES=$((CORPUS_SIZE_MB * 1024 * 1024))
          
          echo "Extracting ${CORPUS_SIZE_MB}MB to ramdisk..."
          head -c "$CORPUS_SIZE_BYTES" corpus_full.txt > /mnt/ramdisk/corpus.txt
          
          # 删除原始大文件
          rm -f corpus_full.txt
          
          echo "Corpus in ramdisk:"
          ls -lh /mnt/ramdisk/corpus.txt
          wc -l /mnt/ramdisk/corpus.txt
          
          echo ""
          echo "Ramdisk usage:"
          df -h /mnt/ramdisk

      - name: Train tokenizer with mimalloc
        env:
          LD_PRELOAD: ${{ env.MIMALLOC_PATH }}
          MIMALLOC_LARGE_OS_PAGES: 1
          VOCAB_SIZE: ${{ github.event.inputs.vocab_size || '8192' }}
        run: |
          echo "Training with mimalloc: $LD_PRELOAD"
          
          python train_tokenizer.py \
            --corpus /mnt/ramdisk/corpus.txt \
            --output ./tokenizer_output \
            --vocab-size $VOCAB_SIZE

      - name: Upload tokenizer artifact
        uses: actions/upload-artifact@v4
        with:
          name: hf-tokenizer-${{ github.event.inputs.vocab_size || '8192' }}
          path: tokenizer_output/
          retention-days: 30
